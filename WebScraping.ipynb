{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information, if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating a URL with a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"https://absolventa.de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_url(position = \"\", location = \"\"):\n",
    "    position = position.lower().replace(\" \", \"+\")\n",
    "    location = location.lower()\n",
    "    search_url = f\"https://www.absolventa.de/jobs?text={position}&location={location}\"\n",
    "    search_url = requests.get(search_url)\n",
    "    \n",
    "    soup = BeautifulSoup(search_url.text)\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract the Job Data from a Single Job Posting Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def job_posting(soup):\n",
    "    postings = soup.find_all(\"a\", class_=\"flex flex-col gap-sm bg-white rounded-xl p-sm border border-1 border-outline hover:border-primary aria-current-location:border-primary text-secondary hover:text-primary active:shadow-input aria-current-location:shadow-input block aria-current-location:cursor-default h-full\")\n",
    "\n",
    "    if postings:\n",
    "        for posting in postings:\n",
    "            try:\n",
    "                title = posting.find(\"h2\", class_=\"text-secondary @lg:hidden break-words hyphens-auto leading-[160%] tracking-tight text-[1rem] font-bold\")\n",
    "                company = posting.find(\"span\", class_=\"text-secondary break-words hyphens-auto leading-[160%] tracking-tight text-[0.875rem]\")\n",
    "                location = posting.find(\"ul\", class_=\"flex flex-wrap flex-col items-start text-xs md:flex-row @md:flex-row gap-xs text-tertiary-500 fill-tertiary-500 md:gap-sm @md:gap-sm\")\n",
    "\n",
    "                results.append({\n",
    "                    \"Job position\": title.text.strip() if title else \"Not available\", \n",
    "                    \"Company name\": company.text.strip() if company else \"Not available\", \n",
    "                    \"Job location\": location.text.strip() if location else \"Not available\"})\n",
    "\n",
    "                print(\"Title:\", title.text.strip() if title else \"Not available\")\n",
    "                print(\"Company:\", company.text.strip() if company else \"Not available\")\n",
    "                print(\"Location:\", location.text.strip() if location else \"Not available\")\n",
    "\n",
    "                print(\"+\" * 40)\n",
    "            \n",
    "            except AttributeError as e:\n",
    "                print(f\"Error parsing a job posting: {e}\")\n",
    "                results.append({\n",
    "                    \"Job position\": \"Error\",\n",
    "                    \"Company name\": \"Error\",\n",
    "                    \"Job location\": \"Error\"\n",
    "                })\n",
    "    else:\n",
    "        print(f\"There are currently no job openings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(position, location):\n",
    "    soup = generate_url(position, location)\n",
    "    job_posting(soup)\n",
    "\n",
    "    with open(\"job_postings_result.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        fieldnames = [\"Job position\", \"Company name\", \"Job location\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(position=\"data analyst\", location=\"berlin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Conclusions\n",
    "\n",
    "- It was challenging to find a job posting website that wasn't anti-scraping. When I attempted to scrape sites like Indeed, Monster, Glassdoor etc., I consistently encountered a 403 error code. After researching these errors, I came across several suggestions, such as using headers, proxies, and other techniques. However, none of these solutions worked for me. Eventually, after some additional research I decided to move on with Germany based job posting site [absolventa.de](https://absolventa.de), which proved to be very useful and has straightforward URL parameters.\n",
    "- Then I defined three functions: `generate_url`, `job_posting` and `main`. The first two functions are called within the `main` function, and I divided each processes for better flexibility. The first function, `generate_url`, creates the URL using parameters provided by the user and returns a `soup` object. The second function, `job_posting`, takes this `soup` as input, parses HTML, and retrieves all job postings, if available. Finally, the `main` function orchestrates the process by calling the other two functions, making it simpler and more user-friendly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
